# -*- coding: utf-8 -*-
"""assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xlal7-Cy9Pwnk33Skcl7DvYGkNCOTbuj

# Assignment 2 - Introduction to NLTK

In part 1 of this assignment you will use nltk to explore the <a href='http://www.cs.cmu.edu/~ark/personas/'>CMU Movie Summary Corpus</a>. All data is released under a <a href='https://creativecommons.org/licenses/by-sa/3.0/us/legalcode'>Creative Commons Attribution-ShareAlike License</a>. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling.

## Part 1 - Analyzing Plots Summary Text
"""

import nltk
import pandas as pd
import numpy as np

nltk.data.path.append("assets/")

# If you would like to work with the raw text you can use 'plots_raw'
with open('assets/plots.txt', 'rt', encoding="utf8") as f:
    plots_raw = f.read()

# If you would like to work with the plot summaries in nltk.Text format you can use 'text1'.
plots_tokens = nltk.word_tokenize(plots_raw)
text1 = nltk.Text(plots_tokens)

"""### Example 1

How many tokens (words and punctuation symbols) are in text1?

*This function should return an integer.*
"""

def example_one():

    return len(nltk.word_tokenize(plots_raw)) # or alternatively len(text1)

example_one()

"""### Example 2

How many unique tokens (unique words and punctuation) does text1 have?

*This function should return an integer.*
"""

def example_two():

    return len(set(nltk.word_tokenize(plots_raw))) # or alternatively len(set(text1))

example_two()

"""### Example 3

After lemmatizing the verbs, how many unique tokens does text1 have?

*This function should return an integer.*
"""

from nltk.stem import WordNetLemmatizer

def example_three():

    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]

    return len(set(lemmatized))

example_three()

"""### Question 1

What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)

*This function should return a float.*
"""

def answer_one():

    unique = len(set(text1))
    total = len(text1)
    return unique / total

answer_one()



"""### Question 2

What percentage of tokens is 'love'or 'Love'?

*This function should return a float.*
"""

def answer_two():

    count = text1.count('love') + text1.count('Love')
    total = len(text1)
    return (count / total) * 100

answer_two()



"""### Question 3

What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?

*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*
"""

def answer_three():

    from collections import Counter
    freq = Counter(text1)
    return freq.most_common(20)

answer_three()



"""### Question 4

What tokens have a length of greater than 5 and frequency of more than 200?

*This function should return an alphabetically sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*
"""

def answer_four():

    from collections import Counter
    freq = Counter(text1)
    tokens = [word for word, count in freq.items() if len(word) > 5 and count > 200]
    return sorted(tokens)

answer_four()



"""### Question 5

Find the longest token in text1 and that token's length.

*This function should return a tuple `(longest_word, length)`.*
"""

def answer_five():

    longest = max(text1, key=len)
    return (longest, len(longest))

answer_five()



"""### Question 6

What unique words have a frequency of more than 2000? What is their frequency?

"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation."

*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*
"""

def answer_six():

    from collections import Counter
    freq = Counter(text1)
    words = [(count, word) for word, count in freq.items() if word.isalpha() and count > 2000]
    return sorted(words, reverse=True)

answer_six()



"""### Question 7

`text1` is in `nltk.Text` format that has been constructed using tokens output by `nltk.word_tokenize(plots_raw)`.

Now, use `nltk.sent_tokenize` on the tokens in `text1` by joining them using whitespace to output a sentence-tokenized copy of `text1`. Report the average number of whitespace separated tokens per sentence in the sentence-tokenized copy of `text1`.

*This function should return a float.*
"""

def answer_seven():

    text = ' '.join(text1)
    sentences = nltk.sent_tokenize(text)
    num_sentences = len(sentences)
    num_tokens = len(text1)
    return num_tokens / num_sentences

answer_seven()



"""### Question 8

What are the 5 most frequent parts of speech in `text1`? What is their frequency?

*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*
"""

def answer_eight():

    tags = nltk.pos_tag(text1)
    from collections import Counter
    counts = Counter(tag for word, tag in tags)
    return counts.most_common(5)

answer_eight()



"""## Part 2 - Spelling Recommender

For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.

For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.

*Each of the three different recommenders will use a different distance measure (outlined below).

Each of the recommenders should provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`.
"""

from nltk.corpus import words

correct_spellings = words.words()

"""### Question 9

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**

Refer to:
- [NLTK Jaccard distance](https://www.nltk.org/api/nltk.metrics.distance.html?highlight=jaccard_distance#nltk.metrics.distance.jaccard_distance)
- [NLTK ngrams](https://www.nltk.org/api/nltk.util.html?highlight=ngrams#nltk.util.ngrams)

*This function should return a list of length three:
`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*
"""

from nltk.metrics.distance import jaccard_distance
from nltk.util import ngrams

def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):
    recommendations = []
    for word in entries:
        # Filter correct words that start with the same first letter
        candidates = [w for w in correct_spellings if w.startswith(word[0])]
        # Compute Jaccard distance for each candidate
        distances = [
            (jaccard_distance(set(ngrams(word, 3)), set(ngrams(cand, 3))), cand)
            for cand in candidates
        ]
        # Find the candidate with the minimum distance
        recommendation = min(distances)[1]
        recommendations.append(recommendation)
    return recommendations

answer_nine()



"""### Question 10

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**

Refer to:
- [NLTK Jaccard distance](https://www.nltk.org/api/nltk.metrics.distance.html?highlight=jaccard_distance#nltk.metrics.distance.jaccard_distance)
- [NLTK ngrams](https://www.nltk.org/api/nltk.util.html?highlight=ngrams#nltk.util.ngrams)

*This function should return a list of length three:
`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*
"""

def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):
    recommendations = []
    for word in entries:
        candidates = [w for w in correct_spellings if w.startswith(word[0])]
        distances = [
            (jaccard_distance(set(ngrams(word, 4)), set(ngrams(cand, 4))), cand)
            for cand in candidates
        ]
        recommendation = min(distances)[1]
        recommendations.append(recommendation)
    return recommendations

answer_ten()



"""### Question 11

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**

Refer to:
- [NLTK edit distance](https://www.nltk.org/api/nltk.metrics.distance.html?highlight=edit_distance#nltk.metrics.distance.edit_distance)

*This function should return a list of length three:
`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*
"""

from nltk.metrics.distance import edit_distance

def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):
    recommendations = []
    for word in entries:
        candidates = [w for w in correct_spellings if w.startswith(word[0])]
        distances = [
            (edit_distance(word, cand), cand)
            for cand in candidates
        ]
        recommendation = min(distances)[1]
        recommendations.append(recommendation)
    return recommendations

answer_eleven()

